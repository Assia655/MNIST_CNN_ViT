{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms, models\nimport time\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport struct\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\n# ==================== DEVICE SETUP ====================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ==================== CUSTOM MNIST LOADER ====================\ndef load_mnist_images(filepath):\n    \"\"\"Charge les images MNIST depuis un fichier .ubyte\"\"\"\n    with open(filepath, 'rb') as f:\n        magic, num_images, rows, cols = struct.unpack('>4I', f.read(16))\n        data = np.frombuffer(f.read(), dtype=np.uint8)\n        return data.reshape(num_images, rows, cols)\n\ndef load_mnist_labels(filepath):\n    \"\"\"Charge les labels MNIST depuis un fichier .ubyte\"\"\"\n    with open(filepath, 'rb') as f:\n        magic, num_labels = struct.unpack('>2I', f.read(8))\n        data = np.frombuffer(f.read(), dtype=np.uint8)\n        return data\n\nclass MNISTDataset(Dataset):\n    \"\"\"Dataset personnalisé pour MNIST\"\"\"\n    def __init__(self, images, labels, transform=None):\n        self.images = torch.FloatTensor(images).unsqueeze(1) / 255.0\n        self.labels = torch.LongTensor(labels)\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        img = self.images[idx]\n        label = self.labels[idx]\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n\n# ==================== DATA LOADING ====================\nprint(\"\\nChargement du dataset MNIST...\")\n\ntransform = transforms.Compose([\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# Essayer de charger depuis Kaggle\nkaggle_path = '/kaggle/input/mnist-dataset/mnist'\nlocal_path = './data'\n\ntry:\n    train_images = load_mnist_images(f'{kaggle_path}/train-images-idx3-ubyte')\n    train_labels = load_mnist_labels(f'{kaggle_path}/train-labels-idx1-ubyte')\n    test_images = load_mnist_images(f'{kaggle_path}/t10k-images-idx3-ubyte')\n    test_labels = load_mnist_labels(f'{kaggle_path}/t10k-labels-idx1-ubyte')\n    \n    train_dataset = MNISTDataset(train_images, train_labels, transform=transform)\n    test_dataset = MNISTDataset(test_images, test_labels, transform=transform)\n    print(\"✓ Dataset chargé depuis Kaggle\")\nexcept FileNotFoundError:\n    try:\n        train_images = load_mnist_images(f'{local_path}/train-images-idx3-ubyte')\n        train_labels = load_mnist_labels(f'{local_path}/train-labels-idx1-ubyte')\n        test_images = load_mnist_images(f'{local_path}/t10k-images-idx3-ubyte')\n        test_labels = load_mnist_labels(f'{local_path}/t10k-labels-idx1-ubyte')\n        \n        train_dataset = MNISTDataset(train_images, train_labels, transform=transform)\n        test_dataset = MNISTDataset(test_images, test_labels, transform=transform)\n        print(\"✓ Dataset chargé depuis ./data\")\n    except FileNotFoundError:\n        print(\"Téléchargement depuis torchvision...\")\n        train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n        test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n        print(\"✓ Dataset téléchargé\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\n# ==================== PART 2: VISION TRANSFORMER FROM SCRATCH ====================\nprint(\"\\n\" + \"=\"*70)\nprint(\"PART 2: VISION TRANSFORMER (ViT) FROM SCRATCH\")\nprint(\"=\"*70)\n\nclass PatchEmbedding(nn.Module):\n    \"\"\"Convertit une image en patches et les embed\"\"\"\n    def __init__(self, img_size=28, patch_size=4, in_channels=1, embed_dim=256):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n    \n    def forward(self, x):\n        # x: (B, C, H, W)\n        x = self.proj(x)  # (B, embed_dim, H/patch_size, W/patch_size)\n        x = rearrange(x, 'b e h w -> b (h w) e')  # (B, num_patches, embed_dim)\n        return x\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-Head Self-Attention\"\"\"\n    def __init__(self, embed_dim=256, num_heads=8, attn_drop=0.0, proj_drop=0.0):\n        super().__init__()\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n    \n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        \n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer Block = MHA + MLP\"\"\"\n    def __init__(self, embed_dim=256, num_heads=8, mlp_dim=512, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout, dropout)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        \n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nclass VisionTransformer(nn.Module):\n    \"\"\"Vision Transformer\"\"\"\n    def __init__(self, img_size=28, patch_size=4, num_classes=10, \n                 embed_dim=256, num_heads=8, depth=12, mlp_dim=512, dropout=0.1):\n        super().__init__()\n        \n        # Patch Embedding\n        self.patch_embed = PatchEmbedding(img_size, patch_size, 1, embed_dim)\n        num_patches = self.patch_embed.num_patches\n        \n        # Class token et position embedding\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(dropout)\n        \n        # Transformer blocks\n        self.transformer = nn.Sequential(*[\n            TransformerBlock(embed_dim, num_heads, mlp_dim, dropout)\n            for _ in range(depth)\n        ])\n        \n        # Classification head\n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n        \n        # Initialize weights\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n    \n    def forward(self, x):\n        B = x.shape[0]\n        \n        # Patch embedding\n        x = self.patch_embed(x)  # (B, num_patches, embed_dim)\n        \n        # Add class token\n        cls_token = repeat(self.cls_token, '1 1 d -> b 1 d', b=B)\n        x = torch.cat([cls_token, x], dim=1)  # (B, num_patches+1, embed_dim)\n        \n        # Add position embedding\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        \n        # Transformer blocks\n        x = self.transformer(x)\n        \n        # Classification\n        x = self.norm(x)\n        x = x[:, 0]  # Take class token\n        x = self.head(x)\n        \n        return x\n\ndef train_vit(model, train_loader, test_loader, num_epochs=10):\n    \"\"\"Entraîne le Vision Transformer\"\"\"\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n    \n    start_time = time.time()\n    train_losses, test_losses, train_accs, test_accs = [], [], [], []\n    \n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        running_loss = 0\n        correct = 0\n        total = 0\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        train_loss = running_loss / len(train_loader)\n        train_acc = correct / total\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        \n        # Testing\n        model.eval()\n        test_loss = 0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for images, labels in test_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                test_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        test_loss = test_loss / len(test_loader)\n        test_acc = correct / total\n        test_losses.append(test_loss)\n        test_accs.append(test_acc)\n        \n        scheduler.step()\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%, \"\n              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc*100:.2f}%\")\n    \n    training_time = time.time() - start_time\n    return model, train_losses, test_losses, train_accs, test_accs, training_time\n\ndef evaluate_model(model, test_loader):\n    \"\"\"Évalue le modèle\"\"\"\n    model.eval()\n    all_preds, all_labels = [], []\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return accuracy, f1, all_preds, all_labels\n\n# Train ViT\nprint(\"\\n--- Vision Transformer Training ---\")\nvit_model = VisionTransformer(\n    img_size=28,\n    patch_size=4,\n    num_classes=10,\n    embed_dim=256,\n    num_heads=8,\n    depth=12,\n    mlp_dim=512,\n    dropout=0.1\n)\n\nvit_model, vit_train_loss, vit_test_loss, vit_train_acc, vit_test_acc, vit_time = train_vit(\n    vit_model, train_loader, test_loader, num_epochs=10\n)\n\nvit_final_acc, vit_f1, vit_preds, vit_labels = evaluate_model(vit_model, test_loader)\n\nprint(f\"\\nVision Transformer Final Results:\")\nprint(f\"Accuracy: {vit_final_acc:.4f}\")\nprint(f\"F1 Score: {vit_f1:.4f}\")\nprint(f\"Training Time: {vit_time:.2f}s\")\nprint(f\"Final Test Loss: {vit_test_loss[-1]:.4f}\")\n\n# ==================== PART 3: COMPARISON ALL MODELS ====================\nprint(\"\\n\" + \"=\"*70)\nprint(\"PART 3: COMPREHENSIVE COMPARISON - ALL MODELS\")\nprint(\"=\"*70)\n\n# Résultats de la Part 1 (vous pouvez les charger ou les entrer manuellement)\n# Pour cette démo, nous allons créer des modèles simples de Part 1\n\n# CNN Simple\nclass CNN(nn.Module):\n    def __init__(self, in_channels=1, num_classes=10):\n        super(CNN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 7 * 7, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\ndef quick_train(model, train_loader, test_loader, epochs=5, lr=0.001):\n    \"\"\"Entraînement rapide pour comparaison\"\"\"\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    start_time = time.time()\n    \n    for epoch in range(epochs):\n        model.train()\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        \n        if (epoch + 1) % epochs == 0:\n            model.eval()\n            with torch.no_grad():\n                for images, labels in test_loader:\n                    break\n    \n    training_time = time.time() - start_time\n    \n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return accuracy, f1, training_time\n\nprint(\"\\nEntraînement des modèles de Part 1 pour comparaison...\")\ncnn_model = CNN()\ncnn_acc, cnn_f1, cnn_time = quick_train(cnn_model, train_loader, test_loader, epochs=5)\nprint(f\"CNN - Accuracy: {cnn_acc:.4f}, F1: {cnn_f1:.4f}, Time: {cnn_time:.2f}s\")\n\n# ==================== FINAL COMPARISON TABLE ====================\nprint(\"\\n\" + \"=\"*70)\nprint(\"COMPARISON RESULTS\")\nprint(\"=\"*70)\n\ncomparison_results = {\n    'CNN': {\n        'Accuracy': cnn_acc,\n        'F1 Score': cnn_f1,\n        'Training Time': cnn_time,\n        'Parameters': sum(p.numel() for p in cnn_model.parameters())\n    },\n    'Vision Transformer': {\n        'Accuracy': vit_final_acc,\n        'F1 Score': vit_f1,\n        'Training Time': vit_time,\n        'Parameters': sum(p.numel() for p in vit_model.parameters())\n    }\n}\n\nprint(f\"\\n{'Model':<20} {'Accuracy':<15} {'F1 Score':<15} {'Time(s)':<15} {'Parameters':<15}\")\nprint(\"-\" * 85)\nfor model_name, metrics in comparison_results.items():\n    print(f\"{model_name:<20} {metrics['Accuracy']:.4f}         {metrics['F1 Score']:.4f}         \"\n          f\"{metrics['Training Time']:.2f}         {metrics['Parameters']:<15}\")\n\n# ==================== INTERPRETATION & ANALYSIS ====================\nprint(\"\\n\" + \"=\"*70)\nprint(\"INTERPRETATION & ANALYSIS\")\nprint(\"=\"*70)\n\nprint(\"\\n1. ACCURACY COMPARISON:\")\nacc_diff = vit_final_acc - cnn_acc\nprint(f\"   Vision Transformer: {vit_final_acc:.4f}\")\nprint(f\"   CNN: {cnn_acc:.4f}\")\nprint(f\"   Difference: {acc_diff:+.4f} {'(ViT Better)' if acc_diff > 0 else '(CNN Better)'}\")\n\nprint(\"\\n2. COMPUTATIONAL EFFICIENCY:\")\ntime_diff = vit_time - cnn_time\nprint(f\"   Vision Transformer: {vit_time:.2f}s\")\nprint(f\"   CNN: {cnn_time:.2f}s\")\nprint(f\"   Time Difference: {time_diff:+.2f}s\")\n\nprint(\"\\n3. MODEL COMPLEXITY:\")\nvit_params = sum(p.numel() for p in vit_model.parameters())\ncnn_params = sum(p.numel() for p in cnn_model.parameters())\nprint(f\"   Vision Transformer: {vit_params:,} parameters\")\nprint(f\"   CNN: {cnn_params:,} parameters\")\nprint(f\"   Ratio: {vit_params/cnn_params:.2f}x\")\n\nprint(\"\\n4. KEY INSIGHTS:\")\nprint(\"   • Vision Transformers capture global dependencies via self-attention\")\nprint(\"   • CNNs are more efficient for small images like MNIST (28x28)\")\nprint(\"   • ViT requires more data and computation, benefits more from large datasets\")\nprint(\"   • For MNIST: CNN likely performs better due to task simplicity\")\nprint(\"   • ViT architecture is more versatile for complex vision tasks\")\n\n# ==================== VISUALIZATION ====================\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Loss Comparison\naxes[0, 0].plot(range(1, len(vit_train_loss)+1), vit_train_loss, 'b-o', label='ViT Train', linewidth=2)\naxes[0, 0].plot(range(1, len(vit_test_loss)+1), vit_test_loss, 'b--s', label='ViT Test', linewidth=2)\naxes[0, 0].set_title('Vision Transformer - Loss', fontsize=12, fontweight='bold')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Accuracy Comparison\naxes[0, 1].plot(range(1, len(vit_train_acc)+1), [a*100 for a in vit_train_acc], 'g-o', label='ViT Train', linewidth=2)\naxes[0, 1].plot(range(1, len(vit_test_acc)+1), [a*100 for a in vit_test_acc], 'g--s', label='ViT Test', linewidth=2)\naxes[0, 1].set_title('Vision Transformer - Accuracy', fontsize=12, fontweight='bold')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('Accuracy (%)')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Model Comparison Bar Chart\nmodels = list(comparison_results.keys())\naccuracies = [comparison_results[m]['Accuracy'] for m in models]\ncolors = ['#1f77b4', '#ff7f0e']\naxes[1, 0].bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\naxes[1, 0].set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\naxes[1, 0].set_ylabel('Accuracy')\naxes[1, 0].set_ylim([0, 1])\nfor i, v in enumerate(accuracies):\n    axes[1, 0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\naxes[1, 0].grid(True, alpha=0.3, axis='y')\n\n# Training Time Comparison\ntimes = [comparison_results[m]['Training Time'] for m in models]\naxes[1, 1].bar(models, times, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\naxes[1, 1].set_title('Training Time Comparison', fontsize=12, fontweight='bold')\naxes[1, 1].set_ylabel('Time (seconds)')\nfor i, v in enumerate(times):\n    axes[1, 1].text(i, v + max(times)*0.02, f'{v:.2f}s', ha='center', fontweight='bold')\naxes[1, 1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig('vit_comparison.png', dpi=100, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n✓ Vision Transformer Analysis Complete!\")\nprint(\"✓ Comparison visualization saved as 'vit_comparison.png'\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}