{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":242592,"sourceType":"datasetVersion","datasetId":102285}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms, models\nimport time\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\nimport matplotlib.pyplot as plt\nimport struct\nimport os\n\n# ==================== DEVICE SETUP ====================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ==================== CUSTOM MNIST LOADER ====================\ndef load_mnist_images(filepath):\n    \"\"\"Charge les images MNIST depuis un fichier .ubyte\"\"\"\n    with open(filepath, 'rb') as f:\n        magic, num_images, rows, cols = struct.unpack('>4I', f.read(16))\n        data = np.frombuffer(f.read(), dtype=np.uint8)\n        return data.reshape(num_images, rows, cols)\n\ndef load_mnist_labels(filepath):\n    \"\"\"Charge les labels MNIST depuis un fichier .ubyte\"\"\"\n    with open(filepath, 'rb') as f:\n        magic, num_labels = struct.unpack('>2I', f.read(8))\n        data = np.frombuffer(f.read(), dtype=np.uint8)\n        return data\n\nclass MNISTDataset(Dataset):\n    \"\"\"Dataset personnalisé pour MNIST\"\"\"\n    def __init__(self, images, labels, transform=None):\n        self.images = torch.FloatTensor(images).unsqueeze(1) / 255.0\n        self.labels = torch.LongTensor(labels)\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        img = self.images[idx]\n        label = self.labels[idx]\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n\n# ==================== DATA LOADING ====================\nprint(\"\\nChargement du dataset MNIST...\")\n\ntransform = transforms.Compose([\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# Essayer de charger depuis les fichiers Kaggle\nkaggle_path = '/kaggle/input/datasets/mnist-dataset'\nlocal_path = './data'\n\ntry:\n    # Essayer d'abord le chemin Kaggle\n    train_images = load_mnist_images(f'{kaggle_path}/train-images-idx3-ubyte')\n    train_labels = load_mnist_labels(f'{kaggle_path}/train-labels-idx1-ubyte')\n    test_images = load_mnist_images(f'{kaggle_path}/t10k-images-idx3-ubyte')\n    test_labels = load_mnist_labels(f'{kaggle_path}/t10k-labels-idx1-ubyte')\n    \n    train_dataset = MNISTDataset(train_images, train_labels, transform=transform)\n    test_dataset = MNISTDataset(test_images, test_labels, transform=transform)\n    print(\"✓ Dataset chargé depuis Kaggle (/kaggle/input/mnist-dataset/mnist)\")\nexcept FileNotFoundError:\n    try:\n        # Essayer le chemin local\n        train_images = load_mnist_images(f'{local_path}/train-images-idx3-ubyte')\n        train_labels = load_mnist_labels(f'{local_path}/train-labels-idx1-ubyte')\n        test_images = load_mnist_images(f'{local_path}/t10k-images-idx3-ubyte')\n        test_labels = load_mnist_labels(f'{local_path}/t10k-labels-idx1-ubyte')\n        \n        train_dataset = MNISTDataset(train_images, train_labels, transform=transform)\n        test_dataset = MNISTDataset(test_images, test_labels, transform=transform)\n        print(\"✓ Dataset chargé depuis ./data\")\n    except FileNotFoundError:\n        print(\"Fichiers non trouvés, téléchargement depuis torchvision...\")\n        train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n        test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n        print(\"✓ Dataset téléchargé et chargé\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\n# ==================== PART 1: CUSTOM CNN ====================\nprint(\"\\n\" + \"=\"*60)\nprint(\"PART 1: CNN CLASSIFICATION\")\nprint(\"=\"*60)\n\nclass CNN(nn.Module):\n    def __init__(self, in_channels=1, num_classes=10):\n        super(CNN, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 7 * 7, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\ndef train_cnn(model, train_loader, test_loader, num_epochs=5):\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    start_time = time.time()\n    train_losses, test_losses, train_accs, test_accs = [], [], [], []\n    \n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        running_loss = 0\n        correct = 0\n        total = 0\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        train_loss = running_loss / len(train_loader)\n        train_acc = correct / total\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        \n        # Testing\n        model.eval()\n        test_loss = 0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for images, labels in test_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                test_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        test_loss = test_loss / len(test_loader)\n        test_acc = correct / total\n        test_losses.append(test_loss)\n        test_accs.append(test_acc)\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc*100:.2f}%\")\n    \n    training_time = time.time() - start_time\n    return model, train_losses, test_losses, train_accs, test_accs, training_time\n\n# Train CNN\ncnn_model = CNN().to(device)\ncnn_model, cnn_train_loss, cnn_test_loss, cnn_train_acc, cnn_test_acc, cnn_time = train_cnn(cnn_model, train_loader, test_loader, num_epochs=5)\n\ndef evaluate_model(model, test_loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.numpy())\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return accuracy, f1\n\ncnn_final_acc, cnn_f1 = evaluate_model(cnn_model, test_loader)\nprint(f\"\\nCNN Final Results:\")\nprint(f\"Accuracy: {cnn_final_acc:.4f}\")\nprint(f\"F1 Score: {cnn_f1:.4f}\")\nprint(f\"Training Time: {cnn_time:.2f}s\")\nprint(f\"Final Test Loss: {cnn_test_loss[-1]:.4f}\")\n\n# ==================== PART 2: FASTER R-CNN ====================\nprint(\"\\n\" + \"=\"*60)\nprint(\"PART 2: FASTER R-CNN (Object Detection adapted for Classification)\")\nprint(\"=\"*60)\n\nclass FasterRCNN_Adapted(nn.Module):\n    def __init__(self, num_classes=10):\n        super(FasterRCNN_Adapted, self).__init__()\n        \n        # Backbone: ResNet50 features\n        self.backbone = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            \n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            \n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        # RPN-like region classifier\n        self.region_classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(256 * 1 * 1, 256),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.region_classifier(x)\n        return x\n\ndef train_fasterrcnn(model, train_loader, test_loader, num_epochs=5):\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    start_time = time.time()\n    train_losses, test_losses, train_accs, test_accs = [], [], [], []\n    \n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        running_loss = 0\n        correct = 0\n        total = 0\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n        \n        train_loss = running_loss / len(train_loader)\n        train_acc = correct / total\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        \n        # Testing\n        model.eval()\n        test_loss = 0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for images, labels in test_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                test_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        test_loss = test_loss / len(test_loader)\n        test_acc = correct / total\n        test_losses.append(test_loss)\n        test_accs.append(test_acc)\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc*100:.2f}%\")\n    \n    training_time = time.time() - start_time\n    return model, train_losses, test_losses, train_accs, test_accs, training_time\n\n# Train Faster R-CNN\nfrcnn_model = FasterRCNN_Adapted().to(device)\nfrcnn_model, frcnn_train_loss, frcnn_test_loss, frcnn_train_acc, frcnn_test_acc, frcnn_time = train_fasterrcnn(frcnn_model, train_loader, test_loader, num_epochs=5)\n\nfrcnn_final_acc, frcnn_f1 = evaluate_model(frcnn_model, test_loader)\nprint(f\"\\nFaster R-CNN Final Results:\")\nprint(f\"Accuracy: {frcnn_final_acc:.4f}\")\nprint(f\"F1 Score: {frcnn_f1:.4f}\")\nprint(f\"Training Time: {frcnn_time:.2f}s\")\nprint(f\"Final Test Loss: {frcnn_test_loss[-1]:.4f}\")\n\n# ==================== PART 3: COMPARISON ====================\nprint(\"\\n\" + \"=\"*60)\nprint(\"PART 3: CNN vs FASTER R-CNN COMPARISON\")\nprint(\"=\"*60)\n\ncomparison_data = {\n    'CNN': {\n        'Accuracy': cnn_final_acc,\n        'F1 Score': cnn_f1,\n        'Loss': cnn_test_loss[-1],\n        'Time': cnn_time\n    },\n    'Faster R-CNN': {\n        'Accuracy': frcnn_final_acc,\n        'F1 Score': frcnn_f1,\n        'Loss': frcnn_test_loss[-1],\n        'Time': frcnn_time\n    }\n}\n\nprint(f\"\\n{'Model':<15} {'Accuracy':<15} {'F1 Score':<15} {'Loss':<15} {'Time(s)':<15}\")\nprint(\"-\" * 75)\nfor model_name, metrics in comparison_data.items():\n    print(f\"{model_name:<15} {metrics['Accuracy']:.4f}         {metrics['F1 Score']:.4f}         {metrics['Loss']:.4f}         {metrics['Time']:.2f}\")\n\n# ==================== PART 4: FINE-TUNING PRE-TRAINED MODELS ====================\nprint(\"\\n\" + \"=\"*60)\nprint(\"PART 4: FINE-TUNING VGG16 & ALEXNET\")\nprint(\"=\"*60)\n\n# VGG16 Fine-tuning\nprint(\"\\n--- VGG16 Fine-tuning ---\")\nvgg16_model = models.vgg16(pretrained=True)\n\n# Freeze early layers\nfor param in list(vgg16_model.parameters())[:-4]:\n    param.requires_grad = False\n\n# Modify classifier for MNIST\nvgg16_model.classifier[6] = nn.Linear(4096, 10)\nvgg16_model = vgg16_model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer_vgg = optim.Adam(filter(lambda p: p.requires_grad, vgg16_model.parameters()), lr=0.0001)\n\nstart_time = time.time()\nvgg16_train_loss, vgg16_test_loss, vgg16_train_acc, vgg16_test_acc = [], [], [], []\n\nfor epoch in range(5):\n    vgg16_model.train()\n    running_loss = 0\n    correct = 0\n    total = 0\n    \n    for images, labels in train_loader:\n        images = transforms.Resize((224, 224))(images)  # VGG requires 224x224\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer_vgg.zero_grad()\n        outputs = vgg16_model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer_vgg.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    train_loss = running_loss / len(train_loader)\n    train_acc = correct / total\n    vgg16_train_loss.append(train_loss)\n    vgg16_train_acc.append(train_acc)\n    \n    vgg16_model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = transforms.Resize((224, 224))(images)\n            images, labels = images.to(device), labels.to(device)\n            outputs = vgg16_model(images)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    test_loss = test_loss / len(test_loader)\n    test_acc = correct / total\n    vgg16_test_loss.append(test_loss)\n    vgg16_test_acc.append(test_acc)\n    \n    print(f\"Epoch [{epoch+1}/5] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc*100:.2f}%\")\n\nvgg16_time = time.time() - start_time\nvgg16_final_acc, vgg16_f1 = evaluate_model(vgg16_model, test_loader)\nprint(f\"\\nVGG16 Final Results:\")\nprint(f\"Accuracy: {vgg16_final_acc:.4f}\")\nprint(f\"F1 Score: {vgg16_f1:.4f}\")\nprint(f\"Training Time: {vgg16_time:.2f}s\")\nprint(f\"Final Test Loss: {vgg16_test_loss[-1]:.4f}\")\n\n# AlexNet Fine-tuning\nprint(\"\\n--- AlexNet Fine-tuning ---\")\nalexnet_model = models.alexnet(pretrained=True)\n\n# Freeze early layers\nfor param in list(alexnet_model.parameters())[:-4]:\n    param.requires_grad = False\n\n# Modify classifier\nalexnet_model.classifier[6] = nn.Linear(4096, 10)\nalexnet_model = alexnet_model.to(device)\n\noptimizer_alex = optim.Adam(filter(lambda p: p.requires_grad, alexnet_model.parameters()), lr=0.0001)\n\nstart_time = time.time()\nalex_train_loss, alex_test_loss, alex_train_acc, alex_test_acc = [], [], [], []\n\nfor epoch in range(5):\n    alexnet_model.train()\n    running_loss = 0\n    correct = 0\n    total = 0\n    \n    for images, labels in train_loader:\n        images = transforms.Resize((224, 224))(images)\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer_alex.zero_grad()\n        outputs = alexnet_model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer_alex.step()\n        \n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    train_loss = running_loss / len(train_loader)\n    train_acc = correct / total\n    alex_train_loss.append(train_loss)\n    alex_train_acc.append(train_acc)\n    \n    alexnet_model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images = transforms.Resize((224, 224))(images)\n            images, labels = images.to(device), labels.to(device)\n            outputs = alexnet_model(images)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    test_loss = test_loss / len(test_loader)\n    test_acc = correct / total\n    alex_test_loss.append(test_loss)\n    alex_test_acc.append(test_acc)\n    \n    print(f\"Epoch [{epoch+1}/5] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc*100:.2f}%\")\n\nalex_time = time.time() - start_time\nalex_final_acc, alex_f1 = evaluate_model(alexnet_model, test_loader)\nprint(f\"\\nAlexNet Final Results:\")\nprint(f\"Accuracy: {alex_final_acc:.4f}\")\nprint(f\"F1 Score: {alex_f1:.4f}\")\nprint(f\"Training Time: {alex_time:.2f}s\")\nprint(f\"Final Test Loss: {alex_test_loss[-1]:.4f}\")\n\n# ==================== FINAL COMPARISON ====================\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL COMPARISON: ALL 4 MODELS\")\nprint(\"=\"*60)\n\nfinal_results = {\n    'CNN': {'Accuracy': cnn_final_acc, 'F1': cnn_f1, 'Loss': cnn_test_loss[-1], 'Time': cnn_time},\n    'Faster R-CNN': {'Accuracy': frcnn_final_acc, 'F1': frcnn_f1, 'Loss': frcnn_test_loss[-1], 'Time': frcnn_time},\n    'VGG16': {'Accuracy': vgg16_final_acc, 'F1': vgg16_f1, 'Loss': vgg16_test_loss[-1], 'Time': vgg16_time},\n    'AlexNet': {'Accuracy': alex_final_acc, 'F1': alex_f1, 'Loss': alex_test_loss[-1], 'Time': alex_time}\n}\n\nprint(f\"\\n{'Model':<15} {'Accuracy':<15} {'F1 Score':<15} {'Loss':<15} {'Time(s)':<15}\")\nprint(\"-\" * 75)\nfor model_name, metrics in final_results.items():\n    print(f\"{model_name:<15} {metrics['Accuracy']:.4f}         {metrics['F1']:.4f}         {metrics['Loss']:.4f}         {metrics['Time']:.2f}\")\n\n# ==================== VISUALIZATION ====================\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# CNN Loss\naxes[0, 0].plot(cnn_train_loss, label='Train', marker='o')\naxes[0, 0].plot(cnn_test_loss, label='Test', marker='s')\naxes[0, 0].set_title('CNN - Loss', fontsize=12, fontweight='bold')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].legend()\naxes[0, 0].grid()\n\n# Faster R-CNN Loss\naxes[0, 1].plot(frcnn_train_loss, label='Train', marker='o')\naxes[0, 1].plot(frcnn_test_loss, label='Test', marker='s')\naxes[0, 1].set_title('Faster R-CNN - Loss', fontsize=12, fontweight='bold')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('Loss')\naxes[0, 1].legend()\naxes[0, 1].grid()\n\n# VGG16 Loss\naxes[1, 0].plot(vgg16_train_loss, label='Train', marker='o')\naxes[1, 0].plot(vgg16_test_loss, label='Test', marker='s')\naxes[1, 0].set_title('VGG16 - Loss', fontsize=12, fontweight='bold')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('Loss')\naxes[1, 0].legend()\naxes[1, 0].grid()\n\n# AlexNet Loss\naxes[1, 1].plot(alex_train_loss, label='Train', marker='o')\naxes[1, 1].plot(alex_test_loss, label='Test', marker='s')\naxes[1, 1].set_title('AlexNet - Loss', fontsize=12, fontweight='bold')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Loss')\naxes[1, 1].legend()\naxes[1, 1].grid()\n\nplt.tight_layout()\nplt.savefig('all_models_comparison.png', dpi=100)\nplt.show()\n\nprint(\"\\n✓ Training complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}